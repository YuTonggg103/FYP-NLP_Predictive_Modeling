{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Emoji handling\n",
    "import emoji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Unused Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1_path = r\".\\Ori_Hate_Speech_Dataset\\HateSpeechDataset_Unseen.csv\"\n",
    "\n",
    "df1 = pd.read_csv(file_1_path, encoding='ISO-8859-1', low_memory=False)[['comment', 'label']]\n",
    "data= df1.rename(columns={'comment': 'text'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check NULL & Drop NULL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41108 entries, 0 to 41107\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    41108 non-null  object\n",
      " 1   label   41108 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 642.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows: 10\n",
      "After Remove Dup\n",
      "Number of duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "duplicate_rows = data[data.duplicated()]\n",
    "print(f\"Number of duplicated rows: {duplicate_rows.shape[0]}\")\n",
    "\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "print(\"After Remove Dup\")\n",
    "duplicate_rows = data[data.duplicated()]\n",
    "print(f\"Number of duplicated rows: {duplicate_rows.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                  dalits are lowlives\n",
      "1               gay people are a burden to our society\n",
      "2                                arabs are not welcome\n",
      "3    i'm not saying we should actually eliminate he...\n",
      "4                         bananas are for black people\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data['text'] = data['text'].str.lower()\n",
    "print(data['text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL/HTML/Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URLs\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'(http|https|www)\\S+', '', str(x)))\n",
    "\n",
    "# Remove HTML\n",
    "html_tags_pattern = r'<.*?>'\n",
    "# sub(pattern,replace,text)\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(html_tags_pattern, '', str(x)))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'\\S+html\\b', '', str(x)))\n",
    "\n",
    "# Remove emojis\n",
    "def remove_emojis(text):\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub(r':\\w+:', '', text)\n",
    "    return text\n",
    "data['text'] = data['text'].apply(remove_emojis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common symbol substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common symbol substitutions\n",
    "replace_dict = {\n",
    "    '@': 'a',\n",
    "    '$': 's',\n",
    "    '$$':'ss',\n",
    "    '0': 'o',\n",
    "    '3': 'e',\n",
    "    '1': 'i',\n",
    "    '5': 's',\n",
    "    '7': 't',\n",
    "    '4': 'a',\n",
    "    '9' : 'g',\n",
    "}\n",
    "def replace_symbols(text):\n",
    "    text = str(text)\n",
    "    for symbol, letter in replace_dict.items():\n",
    "        # between the alp: (?<=[A-Za-z])symbol(?=[A-Za-z])\n",
    "        # after the alp：(?<=[A-Za-z])symbol\n",
    "        # before the alp：symbol(?=[A-Za-z])\n",
    "        pattern = rf'(?<=[A-Za-z]){re.escape(symbol)}(?=[A-Za-z])|(?<=[A-Za-z]){re.escape(symbol)}|{re.escape(symbol)}(?=[A-Za-z])'\n",
    "        text = re.sub(pattern, letter, text)\n",
    "    return text\n",
    "\n",
    "data['text'] = data['text'].apply(replace_symbols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Abbreviations 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abbreviations List base on unknow word visualization\n",
    "abbreviations = {\n",
    "    # it's, we're,i'll,\"let's\": \"let us\",\n",
    "    \"it's\":\"it is\",\n",
    "    \"we're\":\"were are\",\n",
    "    \"let's\":\"let us\",\n",
    "    \"i'll\":\"i will\",\n",
    "}\n",
    "# 9. Replace Abbreviations\n",
    "def replace_abbreviations(text):\n",
    "    text = str(text)\n",
    "    for abbr, full_form in abbreviations.items():\n",
    "        text = re.sub(r'\\b' + re.escape(abbr) + r'\\b', full_form, text)\n",
    "    return str(text)\n",
    "data['text'] = data['text'].apply(replace_abbreviations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove ASCII characters/Punctuation/White Space/All Number Rows/Elongation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Remove ASCII characters & delete unused punctuation\n",
    "# df['text'] = df['text'].apply(lambda x: re.sub(r'[^A-Za-z0-9\\s.,!?]', '', str(x)))\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'[^A-Za-z0-9\\s]', '', str(x)))\n",
    "\n",
    "# 12. Remove excessive whitespace\n",
    "# .strip() removes any leading or trailing whitespace from the text\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "\n",
    "# 13.Remove only number rows\n",
    "only_numbers_df = data[data['text'].astype(str).str.strip().str.isdigit()]\n",
    "# print(only_numbers_df)\n",
    "data = data[~data['text'].astype(str).str.strip().str.isdigit()] #turn all data into string (astype(str), remove space (strip()), check is whole string is digit)\n",
    "\n",
    "# 14.Remove repeated punctuation\n",
    "# df['text'] = df['text'].apply(lambda x: re.sub(r'([.!?])\\s*\\1+', r'\\1', x))\n",
    "\n",
    "# 15.Removing elongation (example: goodddddddddd)\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'(.)\\1{2,}', r'\\1\\1', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Abbreviations 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  after remove punctuation\n",
    "\n",
    "# Abbreviations List base on unknow word visualization\n",
    "abbreviations = {\n",
    "    \"auser\" : \"\",\n",
    "    \"werent\":\"were not\",\"arent\": \"are not\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"cant\": \"can not\",\n",
    "    \"shes\": \"she is\",\"hes\": \"he is\",\n",
    "    \"youre\": \"you are\", \n",
    "    \"youll\": \"you will\",\n",
    "    \"youve\": \"you have\",\n",
    "    \"weve\": \"we have\",\n",
    "    \"yall\":\"you all\",\n",
    "    \"theyre\": \"they are\", \n",
    "    \"theyve\": \"they have\",\n",
    "    \"doesnt\": \"does not\", \n",
    "    \"dont\":\"do not\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"im\": \"i am\",\n",
    "    \"iam\": \"i am\",\n",
    "    \"ive\": \"i have\",\n",
    "    \"id\": \"i would\",\n",
    "    \"wth\":\"what the heal\", \"wtf\":\"what the fuck\",\n",
    "    \"fk\":\"fuck\", \"f**k\":\"fuck\",\"fu*k\":\"fuck\", \"f*ck\":\"fuck\",\"fck\":\"fuck\",\"fcking\":\"fucking\",\n",
    "    \"cuz\":\"because\", \"bcuz\":\"because\",\"becuz\":\"because\",\n",
    "    \"bihday\":\"birthday\",\n",
    "    \"etc\":\"et cetera\",\n",
    "    \"selfie\":\"self portrait photograph\",\n",
    "    \"lol\":\"laughing out loud\",\n",
    "    \"lmao\":\"laughing my ass off\",\n",
    "    \"forex\":\" foreign exchange\",\n",
    "    \"lgbt\":\"transgender\",\n",
    "    \"blm\":\"black lives matter\",\n",
    "    \"obama\":\"Barack Obama\",\n",
    "    \"omg\":\"oh my god\",\n",
    "    \"ppl\":\"people\",\n",
    "    \"fathersday\":\"father day\",\n",
    "}\n",
    "# Replace Abbreviations\n",
    "def replace_abbreviations(text):\n",
    "    text = str(text)\n",
    "    for abbr, full_form in abbreviations.items():\n",
    "        text = re.sub(r'\\b' + re.escape(abbr) + r'\\b', full_form, text)\n",
    "    return str(text)\n",
    "data['text'] = data['text'].apply(replace_abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check NUll\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "\n",
      "Missing values per column after dropping NaN:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing values per column:\\n\", data.isnull().sum())\n",
    "data = data.dropna(subset=['text'])\n",
    "\n",
    "print(\"\\nMissing values per column after dropping NaN:\\n\", data.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredTokens = []\n",
    "for token in data['text']:\n",
    "    token = str(token)\n",
    "    wordtokens = nltk.tokenize.word_tokenize(token)\n",
    "    filteredTokens.append(wordtokens)\n",
    "data['text']=filteredTokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert strings back to lists, stored as strings in the CSV file \n",
    "data['text'] = data['text'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x) #(Yadav, 2023)\n",
    "\n",
    "stopTokens = nltk.corpus.stopwords.words(\"english\")\n",
    "stopTokens.remove('not') \n",
    "stopTokens.remove('no') \n",
    "\n",
    "def removeStopWord(words):\n",
    "    return [word for word in words if word.lower() not in stopTokens]\n",
    "data['text'] = data['text'].apply(removeStopWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger') #used for Part-of-Speech (POS) tagging\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_pos_tagging(word):\n",
    "    #[0][1]:('running', 'VBG')\n",
    "    #[0][1][0]:('V')\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV} #need this for wordnet cuz wordnet only have 4 postag\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # Default to noun if no match\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_pos_tagging(word)) for word in text] \n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "data['text'] = data['text'].apply(lemmatize_text)\n",
    "\n",
    "output_file_path = r\".\\Pre_Hate_Dataset\\UnseenData_ForTestSetUsed.csv\"\n",
    "data.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41098 entries, 0 to 41097\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    41098 non-null  object\n",
      " 1   label   41098 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 642.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkNull_file_path =r\".\\Pre_Hate_Dataset\\UnseenData_ForTestSetUsed.csv\"\n",
    "checkNull = pd.read_csv(checkNull_file_path, encoding='ISO-8859-1')\n",
    "checkNull.info()\n",
    "checkNull.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "\n",
      "Missing values per column after dropping NaN:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# drop NULL\n",
    "file_path = r\".\\Pre_Hate_Dataset\\UnseenData_ForTestSetUsed.csv\"\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "print(\"\\nMissing values per column:\\n\", data.isnull().sum())\n",
    "data = data.dropna(subset=['text'])\n",
    "\n",
    "print(\"\\nMissing values per column after dropping NaN:\\n\", data.isnull().sum())\n",
    "\n",
    "output_file =r\".\\Pre_Hate_Dataset\\UnseenData_ForTestSetUsed.csv\"\n",
    "data.to_csv(output_file, index=False, encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows: 460\n",
      "After Remove Dup\n",
      "Number of duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "file_path = r\".\\Pre_Hate_Dataset\\UnseenData_ForTestSetUsed.csv\"\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "duplicate_rows = data[data.duplicated()]\n",
    "print(f\"Number of duplicated rows: {duplicate_rows.shape[0]}\")\n",
    "\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "print(\"After Remove Dup\")\n",
    "duplicate_rows = data[data.duplicated()]\n",
    "print(f\"Number of duplicated rows: {duplicate_rows.shape[0]}\")\n",
    "\n",
    "output_file = r\".\\Pre_Hate_Dataset\\UnseenData_ForTestSetUsed.csv\"\n",
    "data.to_csv(output_file, index=False, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to .\\Pre_Hate_Dataset\\UnseenData_ForTestSetUsed.csv\n"
     ]
    }
   ],
   "source": [
    "# Replace Abbreviations after remove punctuation\n",
    "file_path = r\".\\Pre_Hate_Dataset\\UnseenData_ForTestSetUsed.csv\"\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# Abbreviations List base on unknow word visualization\n",
    "abbreviations = {\n",
    "    \"auser\" : \"\",\n",
    "    \"werent\":\"were not\",\"arent\": \"are not\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"cant\": \"can not\",\n",
    "    \"shes\": \"she is\",\"hes\": \"he is\",\n",
    "    \"youre\": \"you are\", \n",
    "    \"youll\": \"you will\",\n",
    "    \"youve\": \"you have\",\n",
    "    \"weve\": \"we have\",\n",
    "    \"yall\":\"you all\",\n",
    "    \"theyre\": \"they are\", \n",
    "    \"theyve\": \"they have\",\n",
    "    \"doesnt\": \"does not\", \n",
    "    \"dont\":\"do not\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"im\": \"i am\",\n",
    "    \"iam\": \"i am\",\n",
    "    \"ive\": \"i have\",\n",
    "    \"id\": \"i would\",\n",
    "    \"wth\":\"what the heal\", \"wtf\":\"what the fuck\",\n",
    "    \"fk\":\"fuck\", \"f**k\":\"fuck\",\"fu*k\":\"fuck\", \"f*ck\":\"fuck\",\"fck\":\"fuck\",\"fcking\":\"fucking\",\n",
    "    \"cuz\":\"because\", \"bcuz\":\"because\",\"becuz\":\"because\",\n",
    "    \"bihday\":\"birthday\",\n",
    "    \"etc\":\"et cetera\",\n",
    "    \"selfie\":\"self portrait photograph\",\n",
    "    \"lol\":\"laughing out loud\",\n",
    "    \"lmao\":\"laughing my ass off\",\n",
    "    \"forex\":\" foreign exchange\",\n",
    "    \"lgbt\":\"transgender\",\n",
    "    \"blm\":\"black lives matter\",\n",
    "    \"obama\":\"Barack Obama\",\n",
    "    \"omg\":\"oh my god\",\n",
    "    \"ppl\":\"people\",\n",
    "    \"fathersday\":\"father day\",\n",
    "}\n",
    "def replace_abbreviations(text):\n",
    "    text = str(text)\n",
    "    for abbr, full_form in abbreviations.items():\n",
    "        text = re.sub(r'\\b' + re.escape(abbr) + r'\\b', full_form, text)\n",
    "    return str(text)\n",
    "df['text'] = df['text'].apply(replace_abbreviations)\n",
    "\n",
    "output_file_path =r\".\\Pre_Hate_Dataset\\UnseenData_ForTestSetUsed.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Data has been saved to {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
