{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Emoji handling\n",
    "import emoji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import text from file_id File for \"Hate_Speech_Detect\" Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv_path = r\".\\Ori_Hate_Speech_Dataset\\Hate_Speech_Dataset.csv\"\n",
    "file_id_folder = r\".\\HateData_FileID\"\n",
    "output_csv_path = r\".\\Ori_Hate_Speech_Dataset\\Hate_Speech_Dataset_Text.csv\"\n",
    "\n",
    "df = pd.read_csv(input_csv_path, encoding='ISO-8859-1')\n",
    "\n",
    "texts = []\n",
    "for file_id in df['file_id']:\n",
    "    file_path = os.path.join(file_id_folder, f\"{file_id}.txt\")\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "            text = file.read()\n",
    "    else:\n",
    "        text = \"\"\n",
    "    texts.append(text)\n",
    "\n",
    "# create new column for text\n",
    "df['text'] = texts\n",
    "df.to_csv(output_csv_path, index=False, encoding='ISO-8859-1')\n",
    "print(\"Successful Done\", output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Unused Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1_path = r\".\\Ori_Hate_Speech_Dataset\\Dynamically Generated Hate Dataset v0.2.3.csv\"\n",
    "file_2_path = r\".\\Ori_Hate_Speech_Dataset\\Ethos_Dataset_Binary.csv\"\n",
    "file_3_path = r\".\\Ori_Hate_Speech_Dataset\\Hate_Speech_Dataset_Text.csv\"\n",
    "file_4_path = r\".\\Ori_Hate_Speech_Dataset\\Train_Tweet.csv\"\n",
    "\n",
    "\n",
    "output_file_1 = r\".\\Pre_Hate_Dataset\\1_Drop_Unsed_Column\\Dynamically_Drop.csv\"\n",
    "output_file_2 = r\".\\Pre_Hate_Dataset\\1_Drop_Unsed_Column\\Ethos_Drop.csv\"\n",
    "output_file_3 = r\".\\Pre_Hate_Dataset\\1_Drop_Unsed_Column\\Hate_Speech_Dataset_Drop.csv\"\n",
    "output_file_4 = r\".\\Pre_Hate_Dataset\\1_Drop_Unsed_Column\\Train_Tweet_Drop.csv\"\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(file_1_path, encoding='ISO-8859-1', low_memory=False)[['text', 'label']]\n",
    "df1.to_csv(output_file_1, index=False, encoding='ISO-8859-1')\n",
    "\n",
    "df2 = pd.read_csv(file_2_path, encoding='ISO-8859-1', low_memory=False)[['comment', 'isHate']]\n",
    "df2.columns = ['text', 'label']\n",
    "df2.to_csv(output_file_2, index=False, encoding='ISO-8859-1')\n",
    "\n",
    "df3 = pd.read_csv(file_3_path, encoding='ISO-8859-1', low_memory=False)[['text', 'label']]\n",
    "df3.to_csv(output_file_3, index=False, encoding='ISO-8859-1')\n",
    "\n",
    "df4 = pd.read_csv(file_4_path, encoding='ISO-8859-1', low_memory=False)[['tweet', 'label']]\n",
    "df4.columns = ['text', 'label']\n",
    "df4.to_csv(output_file_4, index=False, encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert All Labels Column to Binary Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: .\\Pre_Hate_Dataset\\2_Convert_Label_To_Binary\\Dynamically_Binary.csv\n",
      "Filtered dataset saved to: .\\Pre_Hate_Dataset\\2_Convert_Label_To_Binary\\Hate_Speech_Dataset_Binary.csv\n",
      "Processed and saved: .\\Pre_Hate_Dataset\\2_Convert_Label_To_Binary\\Ethos_Binary.csv\n",
      "                                                  text  label\n",
      "428                                               No .      0\n",
      "429                                   Rule Britannia .      0\n",
      "430  Yes true It reminds me of this incident Muslim...      0\n",
      "431  surely gotta be more than that. town to town y...      0\n",
      "432  Personal Card for Teacher - Johann Warzecha in...      0\n",
      "433  The name of the village was changed to Dramast...      0\n",
      "434                   First I 'm not from Moldavia ...      0\n",
      "435  Second , I really hope one day you chauvinists...      0\n",
      "436  Budapest 1919 , Trianon 1920 , ring a bell ? 2...      0\n",
      "437  There is no way you are taking our ancestral h...      0\n",
      "438                            Oh that 's just great .      0\n",
      "439  I enjoy when a Brit comes in and tells Bulgari...      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_12264\\4138670999.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['label'] = df_filtered['label'].apply(lambda x: 1 if x == 'hate' else 0)\n"
     ]
    }
   ],
   "source": [
    "file_1_path = r\".\\Pre_Hate_Dataset\\1_Drop_Unsed_Column\\Dynamically_Drop.csv\"\n",
    "file_2_path = r\".\\Pre_Hate_Dataset\\1_Drop_Unsed_Column\\Ethos_Drop.csv\"\n",
    "file_3_path = r\".\\Pre_Hate_Dataset\\1_Drop_Unsed_Column\\Hate_Speech_Dataset_Drop.csv\"\n",
    "# file_4_path = r\".\\Pre_Hate_Dataset\\1_Drop_Unsed_Column\\Train_Tweet_Drop.csv\"\n",
    "\n",
    "output_file_1 = r\".\\Pre_Hate_Dataset\\2_Convert_Label_To_Binary\\Dynamically_Binary.csv\"\n",
    "output_file_2 = r\".\\Pre_Hate_Dataset\\2_Convert_Label_To_Binary\\Ethos_Binary.csv\"\n",
    "output_file_3 = r\".\\Pre_Hate_Dataset\\2_Convert_Label_To_Binary\\Hate_Speech_Dataset_Binary.csv\"\n",
    "# output_file_4 =  r\".\\Pre_Hate_Dataset\\2_Convert_Label_To_Binary\\Train_Tweet_Standardize.csv\"\n",
    "\n",
    "\n",
    "def process_label_column_DynamicallyDataset(file_path,output_csv_path):\n",
    "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    df['label'] = df['label'].apply(lambda x: 1 if x == 'hate' else 0) #(Python, n.d.)\n",
    "    df.to_csv(output_csv_path, index=False, encoding='ISO-8859-1')\n",
    "    print(f\"Processed and saved: {output_csv_path}\")\n",
    "process_label_column_DynamicallyDataset(file_1_path,output_file_1)  # Dynamically_Generated_Hate_Dataset\n",
    "\n",
    "def process_label_column_HSDataset(file_path,output_csv_path):\n",
    "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    df_filtered = df[~df['label'].isin([\"idk/skip\", \"relation\"])] #remove label as \"idk/skip\" & \"relation\" \n",
    "    df_filtered['label'] = df_filtered['label'].apply(lambda x: 1 if x == 'hate' else 0)\n",
    "    df_filtered.to_csv(output_csv_path, index=False, encoding='ISO-8859-1')\n",
    "    print(f\"Filtered dataset saved to: {output_csv_path}\")\n",
    "process_label_column_HSDataset(file_3_path,output_file_3)  # Hate_Speech_Dataset_with_Text\n",
    "\n",
    "def process_label_column_ethosDataset(file_path,output_csv_path):\n",
    "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    df['label'] = df['label'].apply(lambda x: 1 if x >=0.5 else 0) \n",
    "    df.to_csv(output_csv_path, index=False, encoding='ISO-8859-1')\n",
    "    print(f\"Processed and saved: {output_csv_path}\")\n",
    "process_label_column_ethosDataset(file_2_path,output_file_2)  # Ethos_Dataset_Binary_Dataset\n",
    "\n",
    "\n",
    "df1_afterConvert = pd.read_csv(output_file_3, encoding='ISO-8859-1')\n",
    "print(df1_afterConvert.iloc[428:440]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine 4 Hate Speech Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame shape: (84807, 2)\n",
      "Combined dataset saved to: .\\Pre_Hate_Dataset\\3_CombineAllHateSpeech_Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "file_1_path = r\".\\Pre_Hate_Dataset\\2_Convert_Label_To_Binary\\Dynamically_Binary.csv\"\n",
    "file_2_path = r\".\\Pre_Hate_Dataset\\2_Convert_Label_To_Binary\\Ethos_Binary.csv\"\n",
    "file_3_path = r\".\\Pre_Hate_Dataset\\2_Convert_Label_To_Binary\\Hate_Speech_Dataset_Binary.csv\"\n",
    "file_4_path =  r\".\\Pre_Hate_Dataset\\2_Convert_Label_To_Binary\\Train_Tweet_Binary.csv\"\n",
    "\n",
    "df1 = pd.read_csv(file_1_path, encoding='ISO-8859-1')\n",
    "df2 = pd.read_csv(file_2_path, encoding='ISO-8859-1')\n",
    "df3 = pd.read_csv(file_3_path, encoding='ISO-8859-1')\n",
    "df4 = pd.read_csv(file_4_path, encoding='ISO-8859-1')\n",
    "\n",
    "combined_df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "print(\"Combined DataFrame shape:\", combined_df.shape)\n",
    "\n",
    "output_file_path = r\".\\Pre_Hate_Dataset\\3_CombineAllHateSpeech_Dataset.csv\"\n",
    "combined_df.to_csv(output_file_path, index=False, encoding='ISO-8859-1')\n",
    "print(f\"Combined dataset saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check NULL & Drop NULL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84807 entries, 0 to 84806\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    84807 non-null  object\n",
      " 1   label   84807 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "file_path = r\".\\Pre_Hate_Dataset\\3_CombineAllHateSpeech_Dataset.csv\"\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "print(data.info())\n",
    "\n",
    "# data = data.dropna(subset=['text'])\n",
    "# output_file_path = r\"C:\\Users\\Asus\\OneDrive\\桌面\\103\\Hate Data Excel\\Text Preprocessing File\\4 Remove\\Remove NULL.csv\"\n",
    "# data.to_csv(output_file_path, index=False, encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                  dalits are lowlives\n",
      "1               gay people are a burden to our society\n",
      "2                                arabs are not welcome\n",
      "3    i'm not saying we should actually eliminate he...\n",
      "4                         bananas are for black people\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "file_path = r\".\\Pre_Hate_Dataset\\3_CombineAllHateSpeech_Dataset.csv\"\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "df['text'] = df['text'].str.lower()\n",
    "print(df['text'].head())\n",
    "\n",
    "output_file_path = r\".\\Pre_Hate_Dataset\\4_Lowercassing.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows: 2611\n",
      "After Remove Dup\n",
      "Number of duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "file_path = r\".\\Pre_Hate_Dataset\\4_Lowercassing.csv\"\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "duplicate_rows = data[data.duplicated()]\n",
    "print(f\"Number of duplicated rows: {duplicate_rows.shape[0]}\")\n",
    "\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "print(\"After Remove Dup\")\n",
    "duplicate_rows = data[data.duplicated()]\n",
    "print(f\"Number of duplicated rows: {duplicate_rows.shape[0]}\")\n",
    "\n",
    "output_file = r\".\\Pre_Hate_Dataset\\5_Remove_Duplicated.csv\"\n",
    "data.to_csv(output_file, index=False, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL/HTML/Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\".\\Pre_Hate_Dataset\\5_Remove_Duplicated.csv\"\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Remove URLs\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'(http|https|www)\\S+', '', str(x)))\n",
    "\n",
    "# Remove HTML\n",
    "html_tags_pattern = r'<.*?>'\n",
    "# sub(pattern,replace,text)\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(html_tags_pattern, '', str(x)))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\S+html\\b', '', str(x)))\n",
    "\n",
    "# Remove emojis\n",
    "def remove_emojis(text):\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub(r':\\w+:', '', text)\n",
    "    return text\n",
    "df['text'] = df['text'].apply(remove_emojis)\n",
    "\n",
    "output_file_path = r\".\\Pre_Hate_Dataset\\6_8_removeURL_HTML_Emoji.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common symbol substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to .\\Pre_Hate_Dataset\\9_Symbol_Substitutions.csv\n"
     ]
    }
   ],
   "source": [
    "# Common symbol substitutions\n",
    "file_path = r\".\\Pre_Hate_Dataset\\6_8_removeURL_HTML_Emoji.csv\"\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "replace_dict = {\n",
    "    '@': 'a',\n",
    "    '$': 's',\n",
    "    '$$':'ss',\n",
    "    '0': 'o',\n",
    "    '3': 'e',\n",
    "    '1': 'i',\n",
    "    '5': 's',\n",
    "    '7': 't',\n",
    "    '4': 'a',\n",
    "    '9' : 'g',\n",
    "}\n",
    "def replace_symbols(text):\n",
    "    text = str(text)\n",
    "    for symbol, letter in replace_dict.items():\n",
    "        # between the alp: (?<=[A-Za-z])symbol(?=[A-Za-z])\n",
    "        # after the alp：(?<=[A-Za-z])symbol\n",
    "        # before the alp：symbol(?=[A-Za-z])\n",
    "        pattern = rf'(?<=[A-Za-z]){re.escape(symbol)}(?=[A-Za-z])|(?<=[A-Za-z]){re.escape(symbol)}|{re.escape(symbol)}(?=[A-Za-z])'\n",
    "        text = re.sub(pattern, letter, text)\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(replace_symbols)\n",
    "\n",
    "output_file_path = r\".\\Pre_Hate_Dataset\\9_Symbol_Substitutions.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Data has been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Abbreviations 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to .\\Pre_Hate_Dataset\\10_Replace_Abbreviations_1.csv\n"
     ]
    }
   ],
   "source": [
    "# 10. Replace Abbreviations\n",
    "file_path = r\".\\Pre_Hate_Dataset\\9_Symbol_Substitutions.csv\"\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# Abbreviations List base on unknow word visualization\n",
    "abbreviations = {\n",
    "    # it's, we're,i'll,\"let's\": \"let us\",\n",
    "    \"it's\":\"it is\",\n",
    "    \"we're\":\"were are\",\n",
    "    \"let's\":\"let us\",\n",
    "    \"i'll\":\"i will\",\n",
    "}\n",
    "# 9. Replace Abbreviations\n",
    "def replace_abbreviations(text):\n",
    "    text = str(text)\n",
    "    for abbr, full_form in abbreviations.items():\n",
    "        text = re.sub(r'\\b' + re.escape(abbr) + r'\\b', full_form, text)\n",
    "    return str(text)\n",
    "df['text'] = df['text'].apply(replace_abbreviations)\n",
    "\n",
    "output_file_path =r\".\\Pre_Hate_Dataset\\10_Replace_Abbreviations_1.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Data has been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove ASCII characters/Punctuation/White Space/All Number Rows/Elongation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to .\\Pre_Hate_Dataset\\11_15_normalize.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None) \n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "file_path =r\".\\Pre_Hate_Dataset\\10_Replace_Abbreviations_1.csv\"\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# 11. Remove ASCII characters & delete unused punctuation\n",
    "# df['text'] = df['text'].apply(lambda x: re.sub(r'[^A-Za-z0-9\\s.,!?]', '', str(x)))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^A-Za-z0-9\\s]', '', str(x)))\n",
    "\n",
    "# 12. Remove excessive whitespace\n",
    "# .strip() removes any leading or trailing whitespace from the text\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "\n",
    "# 13.Remove only number rows\n",
    "only_numbers_df = df[df['text'].astype(str).str.strip().str.isdigit()]\n",
    "# print(only_numbers_df)\n",
    "df = df[~df['text'].astype(str).str.strip().str.isdigit()] #turn all data into string (astype(str), remove space (strip()), check is whole string is digit)\n",
    "\n",
    "# 14.Remove repeated punctuation\n",
    "# df['text'] = df['text'].apply(lambda x: re.sub(r'([.!?])\\s*\\1+', r'\\1', x))\n",
    "\n",
    "# 15.Removing elongation (example: goodddddddddd)\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'(.)\\1{2,}', r'\\1\\1', x))\n",
    "\n",
    "# Save to a new CSV file\n",
    "output_file_path = r\".\\Pre_Hate_Dataset\\11_15_normalize.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Data has been saved to {output_file_path}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Abbreviations 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to .\\Pre_Hate_Dataset\\16_Replace_Abbreviations_2.csv\n"
     ]
    }
   ],
   "source": [
    "#  after remove punctuation\n",
    "file_path = r\".\\Pre_Hate_Dataset\\11_15_normalize.csv\"\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# Abbreviations List base on unknow word visualization\n",
    "abbreviations = {\n",
    "    \"auser\" : \"\",\n",
    "    \"werent\":\"were not\",\"arent\": \"are not\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"cant\": \"can not\",\n",
    "    \"shes\": \"she is\",\"hes\": \"he is\",\n",
    "    \"youre\": \"you are\", \n",
    "    \"youll\": \"you will\",\n",
    "    \"youve\": \"you have\",\n",
    "    \"weve\": \"we have\",\n",
    "    \"yall\":\"you all\",\n",
    "    \"theyre\": \"they are\", \n",
    "    \"theyve\": \"they have\",\n",
    "    \"doesnt\": \"does not\", \n",
    "    \"dont\":\"do not\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"im\": \"i am\",\n",
    "    \"iam\": \"i am\",\n",
    "    \"ive\": \"i have\",\n",
    "    \"id\": \"i would\",\n",
    "    \"wth\":\"what the heal\", \"wtf\":\"what the fuck\",\n",
    "    \"fk\":\"fuck\", \"f**k\":\"fuck\",\"fu*k\":\"fuck\", \"f*ck\":\"fuck\",\"fck\":\"fuck\",\"fcking\":\"fucking\",\n",
    "    \"cuz\":\"because\", \"bcuz\":\"because\",\"becuz\":\"because\",\n",
    "    \"bihday\":\"birthday\",\n",
    "    \"etc\":\"et cetera\",\n",
    "    \"selfie\":\"self portrait photograph\",\n",
    "    \"lol\":\"laughing out loud\",\n",
    "    \"lmao\":\"laughing my ass off\",\n",
    "    \"forex\":\" foreign exchange\",\n",
    "    \"lgbt\":\"transgender\",\n",
    "    \"blm\":\"black lives matter\",\n",
    "    \"obama\":\"Barack Obama\",\n",
    "    \"omg\":\"oh my god\",\n",
    "    \"ppl\":\"people\",\n",
    "    \"fathersday\":\"father day\",\n",
    "}\n",
    "# Replace Abbreviations\n",
    "def replace_abbreviations(text):\n",
    "    text = str(text)\n",
    "    for abbr, full_form in abbreviations.items():\n",
    "        text = re.sub(r'\\b' + re.escape(abbr) + r'\\b', full_form, text)\n",
    "    return str(text)\n",
    "df['text'] = df['text'].apply(replace_abbreviations)\n",
    "\n",
    "output_file_path =r\".\\Pre_Hate_Dataset\\16_Replace_Abbreviations_2.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Data has been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82169 entries, 0 to 82168\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    82119 non-null  object\n",
      " 1   label   82169 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "text     50\n",
       "label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkNull_file_path = r\".\\Pre_Hate_Dataset\\16_Replace_Abbreviations_2.csv\"\n",
    "checkNull = pd.read_csv(output_file_path, encoding='ISO-8859-1')\n",
    "checkNull.info()\n",
    "checkNull.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      " text     50\n",
      "label     0\n",
      "dtype: int64\n",
      "\n",
      "Missing values per column after dropping NaN:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# drop na\n",
    "file_path = r\".\\Pre_Hate_Dataset\\16_Replace_Abbreviations_2.csv\"\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# data.info()\n",
    "print(\"\\nMissing values per column:\\n\", data.isnull().sum())\n",
    "data = data.dropna(subset=['text'])\n",
    "# missing_text_rows = data[data['text'].isnull()]\n",
    "# print(\"Rows with missing 'text':\\n\", missing_text_rows)\n",
    "\n",
    "# data.info()\n",
    "print(\"\\nMissing values per column after dropping NaN:\\n\", data.isnull().sum())\n",
    "\n",
    "output_file =r\".\\Pre_Hate_Dataset\\16_Replace_Abbreviations_2.csv\"\n",
    "data.to_csv(output_file, index=False, encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to .\\Pre_Hate_Dataset\\17_Tokenization.csv\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "file_path = r\".\\Pre_Hate_Dataset\\16_Replace_Abbreviations_2.csv\"\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "filteredTokens = []\n",
    "for token in df['text']:\n",
    "    token = str(token)\n",
    "    wordtokens = nltk.tokenize.word_tokenize(token)\n",
    "    filteredTokens.append(wordtokens)\n",
    "df['text']=filteredTokens\n",
    "\n",
    "output_file_path = r\".\\Pre_Hate_Dataset\\17_Tokenization.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Data has been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to .\\Pre_Hate_Dataset\\18_Remove_StopWord.csv\n"
     ]
    }
   ],
   "source": [
    "# 17. Remove Stop Word\n",
    "file_path = r\".\\Pre_Hate_Dataset\\17_Tokenization.csv\"\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Convert strings back to lists, stored as strings in the CSV file \n",
    "df['text'] = df['text'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x) #(Yadav, 2023)\n",
    "\n",
    "stopTokens = nltk.corpus.stopwords.words(\"english\")\n",
    "stopTokens.remove('not') \n",
    "stopTokens.remove('no') \n",
    "\n",
    "def removeStopWord(words):\n",
    "    return [word for word in words if word.lower() not in stopTokens]\n",
    "df['text'] = df['text'].apply(removeStopWord)\n",
    "\n",
    "output_file_path =r\".\\Pre_Hate_Dataset\\18_Remove_StopWord.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Data has been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger') #used for Part-of-Speech (POS) tagging\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# file_path = r\".\\Pre_Hate_Dataset\\17_Remove_StopWord.csv\"\n",
    "# df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "def get_pos_tagging(word):\n",
    "    #[0][1]:('running', 'VBG')\n",
    "    #[0][1][0]:('V')\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV} #need this for wordnet cuz wordnet only have 4 postag\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # Default to noun if no match\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_pos_tagging(word)) for word in text] \n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "df['text'] = df['text'].apply(lemmatize_text)\n",
    "\n",
    "output_file_path = r\".\\Pre_Hate_Dataset\\19_Lemmatization.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')  # POS tagging\n",
    "nltk.download('punkt')  # for tokenizing text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "file_path = r\".\\Pre_Hate_Dataset\\ForDashBoardUsed.csv\"\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "def get_pos_tagging(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # Default to noun\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # Skip non-string like NaN, float, etc.\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_pos_tagging(word)) for word in tokens]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "df['text'] = df['text'].apply(lemmatize_text)\n",
    "\n",
    "output_file_path = r\".\\Pre_Hate_Dataset\\ForDashBoardUsed.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82119 entries, 0 to 82118\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    82068 non-null  object\n",
      " 1   label   82119 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "text     51\n",
       "label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkNull_file_path =r\".\\Pre_Hate_Dataset\\19_Lemmatization.csv\"\n",
    "checkNull = pd.read_csv(checkNull_file_path, encoding='ISO-8859-1')\n",
    "checkNull.info()\n",
    "checkNull.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      " text     51\n",
      "label     0\n",
      "dtype: int64\n",
      "\n",
      "Missing values per column after dropping NaN:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# drop NULL\n",
    "file_path = r\".\\Pre_Hate_Dataset\\19_Lemmatization.csv\"\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "print(\"\\nMissing values per column:\\n\", data.isnull().sum())\n",
    "data = data.dropna(subset=['text'])\n",
    "\n",
    "print(\"\\nMissing values per column after dropping NaN:\\n\", data.isnull().sum())\n",
    "\n",
    "output_file =r\".\\Pre_Hate_Dataset\\20_Hate_Final.csv\"\n",
    "data.to_csv(output_file, index=False, encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "file_path = r\".\\Pre_Hate_Dataset\\Final_StopWord.csv\"\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "duplicate_rows = data[data.duplicated()]\n",
    "print(f\"Number of duplicated rows: {duplicate_rows.shape[0]}\")\n",
    "\n",
    "# data = data.drop_duplicates()\n",
    "\n",
    "# print(\"After Remove Dup\")\n",
    "# duplicate_rows = data[data.duplicated()]\n",
    "# print(f\"Number of duplicated rows: {duplicate_rows.shape[0]}\")\n",
    "\n",
    "# output_file = r\".\\Pre_Hate_Dataset\\Final_StopWord.csv\"\n",
    "# data.to_csv(output_file, index=False, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to .\\Pre_Hate_Dataset\\Final_StopWord.csv\n"
     ]
    }
   ],
   "source": [
    "# Replace Abbreviations after remove punctuation\n",
    "file_path = r\".\\Pre_Hate_Dataset\\16_Replace_Abbreviations_2.csv\"\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# Abbreviations List base on unknow word visualization\n",
    "abbreviations = {\n",
    "    \"auser\" : \"\",\n",
    "    \"werent\":\"were not\",\"arent\": \"are not\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"cant\": \"can not\",\n",
    "    \"shes\": \"she is\",\"hes\": \"he is\",\n",
    "    \"youre\": \"you are\", \n",
    "    \"youll\": \"you will\",\n",
    "    \"youve\": \"you have\",\n",
    "    \"weve\": \"we have\",\n",
    "    \"yall\":\"you all\",\n",
    "    \"theyre\": \"they are\", \n",
    "    \"theyve\": \"they have\",\n",
    "    \"doesnt\": \"does not\", \n",
    "    \"dont\":\"do not\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"im\": \"i am\",\n",
    "    \"iam\": \"i am\",\n",
    "    \"ive\": \"i have\",\n",
    "    \"id\": \"i would\",\n",
    "    \"wth\":\"what the heal\", \"wtf\":\"what the fuck\",\n",
    "    \"fk\":\"fuck\", \"f**k\":\"fuck\",\"fu*k\":\"fuck\", \"f*ck\":\"fuck\",\"fck\":\"fuck\",\"fcking\":\"fucking\",\n",
    "    \"cuz\":\"because\", \"bcuz\":\"because\",\"becuz\":\"because\",\n",
    "    \"bihday\":\"birthday\",\n",
    "    \"etc\":\"et cetera\",\n",
    "    \"selfie\":\"self portrait photograph\",\n",
    "    \"lol\":\"laughing out loud\",\n",
    "    \"lmao\":\"laughing my ass off\",\n",
    "    \"forex\":\" foreign exchange\",\n",
    "    \"lgbt\":\"transgender\",\n",
    "    \"blm\":\"black lives matter\",\n",
    "    \"obama\":\"Barack Obama\",\n",
    "    \"omg\":\"oh my god\",\n",
    "    \"ppl\":\"people\",\n",
    "    \"fathersday\":\"father day\",\n",
    "}\n",
    "def replace_abbreviations(text):\n",
    "    text = str(text)\n",
    "    for abbr, full_form in abbreviations.items():\n",
    "        text = re.sub(r'\\b' + re.escape(abbr) + r'\\b', full_form, text)\n",
    "    return str(text)\n",
    "df['text'] = df['text'].apply(replace_abbreviations)\n",
    "\n",
    "output_file_path =r\".\\Pre_Hate_Dataset\\Final_StopWord.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Data has been saved to {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
